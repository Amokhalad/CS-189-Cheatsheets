\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,relsize}
\usepackage{color,graphicx,overpic}

% Turn off header and footer
\pagestyle{empty}
\geometry{top=.25in,left=.25in,right=.25in,bottom=.25in}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{\z@}{3ex}{2ex}
											 {\normalfont\large\bfseries\textit}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{\z@}{1ex}{0.5ex}
													{\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{\z@}{-1ex}{0.5ex}
														 {\normalfont\small\bfseries}}
\makeatother

% Don't print section numbers
\setcounter{secnumdepth}{0}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt}

% Math character command.
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\ruler}{\\\rule{8.66cm}{0.1pt}\\}
\newcommand{\norm}[1]{\|#1\|}

% -----------------------------------------------------------------------
\begin{document}
\raggedright
\footnotesize
\begin{multicols*}{3}
% multicol parameters
% These lengths are set only within the two main columns
% \setlength{\columnseprule}{0.1pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{0pt}


\section{Probability}
\textbf{Bayes Theorem}:\\
$P(Y=\pm 1 | X) = \frac{P(X|Y=\pm 1)P(Y=\pm 1)}{P(X|Y= +1)P(Y= +1) + P(X|Y= -1)P(Y= -1)}$
% --------------------------------

\section{Perceptron}
$f(\bx) = \btheta \cdot \bx + \theta_0 = \sum_{i=1}^{d}\theta_ix_i + \theta_0$,
$
\hat{y} =
\begin{cases}
    1,	& \text{if } f(x)\geq 0\\
    -1, & \text{if } f(x) < 0
\end{cases}
$
\ruler
\textbf{Decision boundary}, a \textbf{hyperplane} in $\R^d$: $H = \{\bx \in \R^d : f(\bx) = 0\} = \{\bx \in \R^d : \btheta \cdot \bx + \theta_0 = 0\}$
\ruler
$\btheta$ is the \textbf{normal} of the hyperplane,\\
$\theta_0$ is the \textbf{offset} of the hyperplane from origin,\\
$-\frac{\theta_0}{\norm{\btheta}}$ is the \textbf{signed distance} from the origin to hyperplane.
\ruler
\textbf{Perceptron algorithm},\\
Input: $(\bx_1, y_1),\dots,(\bx_n, y_n) \in \R^{d} \times \{\pm 1\}$\\
while some $y_i \neq \text{sign}(\btheta \cdot \bx_i)$\\
\-\hspace{0.5cm} pick some misclassified $(\bx_i, y_i)$\\
\-\hspace{0.5cm} $\btheta \leftarrow \btheta + y_i\bx_i$
\ruler
Given a \textbf{linearly separable data}, perceptron algorithm will take no more than $\frac{R^2}{\gamma^2}$ updates to \textbf{converge},
where $R = \max_{i}{\norm{\bx_i}}$ is the radius of the data, $\gamma = \min_{i}{\frac{y_i(\btheta\cdot\bx_i)}{\norm{\btheta}}}$ is the margin.\\
Also, $\frac{\btheta\cdot\bx}{\norm{\btheta}}$ is the signed distance from H to $\bx$ in the direction $\btheta$.
\ruler
$\btheta = \sum_{i} \alpha_i y_i \bx_i$, thus any inner product space will work, this is a \textbf{kernel}.
\ruler
\textbf{Gradient descent} view of perceptron, minimize margin cost function
$J(\btheta) = \sum_{i}(-y_i(\btheta\cdot\bx_i))_{+}$ with $\btheta \leftarrow \btheta - \eta\nabla J(\btheta)$
% --------------------------------

\section{Support Vector Machine}
\textbf{Hard margin SVM},\\
$\min_{\btheta} \norm{\btheta}^2$, such that $y_i\btheta\cdot\bx_i \geq 1 (i = 1,\dots,n)$\\
\textbf{Soft margin SVM},\\
$\min_{\btheta} \norm{\btheta}^2 + C\sum_{i=1}^{n}(1-y_i\btheta\cdot\bx_i)_+$
\ruler
\textbf{Regularization and SVMs}:
Simulated data with many features $\phi(\bx)$;
C controls trade-off between margin $1/\norm{\btheta}$ and fit to data;
Large C: focus on fit to data (small margin is ok). More overfitting.
Small C: focus on large margin, less tendency to overfit.
Overfitting increases with: less data, more features.
\ruler
$\btheta = \sum_{j}\alpha_j y_j\bx_j$, $\alpha_j \neq 0$ only for support vectors.
\ruler
$K(\bx_i, \bx_j) = \phi(\bx_i)\cdot\phi(\bx_j)$, K is called a kernel.\\
Solve $\alpha_j$ to determine $\sum_j\alpha_j y_j\phi(\bx_j)$.\\
Compute the classifier for a test point $\bx$ via $\btheta\cdot\phi(\bx) = \sum_{j}\alpha_j y_j K(\bx_j, \bx)$
\ruler
degree-m polynomial kernel: $K_m(\bx, \tilde{\bx}) = (1+\bx\cdot\tilde{\bx})^m$\\
radial basis function kernel: $K_{rbf}(\bx, \tilde{\bx}) = \text{exp}(-\gamma\norm{\bx-\tilde{\bx}}^2)$
% --------------------------------

\section{Decision Theory}
\textbf{Loss function}: $l : \mathcal{Y} \times \mathcal{Y} \rightarrow \R$,
and $l(\hat{y}, y)$ is the cost of predicting $\hat{y}$ when the outcome is $y$.
\ruler
Assume $(\bX, \bY)$ are chosen i.i.d according to some probability distribution on $\mathcal{X}\times\mathcal{Y}$.
\textbf{Risk} is misclassification probability: $R(f) = \E l(f(\bX), \bY) = Pr(f(\bX) \neq \bY)$
\ruler
\textbf{Bayes Decision Rule} is
$
f^{*}(x) =
\begin{cases}
    1,	& \text{if } P(\bY=1|x) > P(\bY=-1|x) \\
    -1, & \text{otherwise. }
\end{cases}
$,\\
and the optimal risk (Bayes risk) $R^{*} = \inf_{f}R(f)=R(f^*)$
\ruler
\textbf{Excess risk} is for any $f: \mathcal{X} \rightarrow \{-1, +1\}$,\\
$R(f) - R^* = \E (1[f(x)\neq f^*(x)]|2P(\bY=+1|\bX) - 1|)$
\ruler
\textbf{Risk in Regression} is expected  squared error:
$R(f) = \E l(f(\bX), \bY) = \E \E [f(\bX) - \bY^2 | \bX]$
\ruler
\textbf{Bias-variance decomposition}:
$R(f) = \E[\underbrace{(f(\bX) - \E[\bY|\bX])^2}_{\text{bias}^2}] + \E[\underbrace{(\E[\bY|\bX]-\bY)^2}_{\text{variance}}]$
% --------------------------------

\section{Generative and Discriminative}
\textbf{Discriminative models}: $P(\bX, \bY) = P(\bX)P(\bY|\bX)$.\\
Estimate $P(\bY|\bX)$, then pretend out estimate $\hat{P}(\bY|\bX)$ is the actual $P(\bY|\bX)$ and
plug in bayes rule expression.
\ruler
\textbf{Generative model}: $P(\bX, \bY) = P(\bY)P(\bX|\bY)$.\\
Estimate $P(\bY)$ and $P(\bX|\bY)$, then use bayes theorem to calculate $P(\bY|\bX)$ and use discriminative model.
% --------------------------------

\section{Estimation}
% \section{Gaussian}
% \section{Regression Regularization}
% \section{Logistic Regression}
% \section{Linear Discriminant Analysis}
% \section{Convex Optimization}

\end{multicols*}
\end{document}
