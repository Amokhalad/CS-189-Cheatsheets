\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,relsize}
\usepackage{color,graphicx,overpic}

% Turn off header and footer
\pagestyle{empty}
\geometry{top=.25in,left=.25in,right=.25in,bottom=.25in}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{\z@}{3ex}{2ex}
                       {\normalfont\large\bfseries\textit}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{\z@}{1ex}{0.5ex}
                          {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{\z@}{-1ex}{0.5ex}
                             {\normalfont\small\bfseries}}
\makeatother

% Don't print section numbers
\setcounter{secnumdepth}{0}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt}

% Math character command.
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}

\newcommand{\bb}{\mathbf{b}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bv}{\mathbf{v}}

\newcommand{\bA}{\mathbf{A}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{\mathbf{Z}}

\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bLambda}{\boldsymbol{\Lambda}}

\newcommand{\tp}[1]{#1^{T}}
\newcommand{\inv}[1]{#1^{-1}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\gaussian}[2]{\mathcal{N}(#1, #2)}

\newcommand{\ruler}{\\\rule{\columnwidth}{0.25pt}\\}

% -----------------------------------------------------------------------
\begin{document}
\raggedright
\footnotesize
\begin{multicols*}{3}
% multicol parameters
% These lengths are set only within the two main columns
% \setlength{\columnseprule}{0.1pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{0pt}


\section{Probability}
\textbf{Bayes Theorem}:\\
$P(Y=\pm 1 | X) = \frac{P(X|Y=\pm 1)P(Y=\pm 1)}{P(X|Y= +1)P(Y= +1) + P(X|Y= -1)P(Y= -1)}$
% --------------------------------

\section{Perceptron}
$f(\bx) = \btheta \cdot \bx + \theta_0 = \sum_{i=1}^{d}\theta_ix_i + \theta_0$,
$
\hat{y} =
\begin{cases}
    1,  & \text{if } f(x)\geq 0\\
    -1, & \text{if } f(x) < 0
\end{cases}
$
\ruler
\textbf{Decision boundary}, a \textbf{hyperplane} in $\R^d$: $H = \{\bx \in \R^d : f(\bx) = 0\} = \{\bx \in \R^d : \btheta \cdot \bx + \theta_0 = 0\}$
\ruler
$\btheta$ is the \textbf{normal} of the hyperplane,\\
$\theta_0$ is the \textbf{offset} of the hyperplane from origin,\\
$-\frac{\theta_0}{\norm{\btheta}}$ is the \textbf{signed distance} from the origin to hyperplane.
\ruler
\textbf{Perceptron algorithm},\\
Input: $(\bx_1, y_1),\dots,(\bx_n, y_n) \in \R^{d} \times \{\pm 1\}$\\
while some $y_i \neq \text{sign}(\btheta \cdot \bx_i)$\\
\-\hspace{0.5cm} pick some misclassified $(\bx_i, y_i)$\\
\-\hspace{0.5cm} $\btheta \leftarrow \btheta + y_i\bx_i$
\ruler
Given a \textbf{linearly separable data}, perceptron algorithm will take no more than $\frac{R^2}{\gamma^2}$ updates to \textbf{converge},
where $R = \max_{i}{\norm{\bx_i}}$ is the radius of the data, $\gamma = \min_{i}{\frac{y_i(\btheta\cdot\bx_i)}{\norm{\btheta}}}$ is the margin.\\
Also, $\frac{\btheta\cdot\bx}{\norm{\btheta}}$ is the signed distance from H to $\bx$ in the direction $\btheta$.
\ruler
$\btheta = \sum_{i} \alpha_i y_i \bx_i$, thus any inner product space will work, this is a \textbf{kernel}.
\ruler
\textbf{Gradient descent} view of perceptron, minimize margin cost function
$J(\btheta) = \sum_{i}(-y_i(\btheta\cdot\bx_i))_{+}$ with $\btheta \leftarrow \btheta - \eta\nabla J(\btheta)$
% --------------------------------

\section{Support Vector Machine}
\textbf{Hard margin SVM},\\
$\min_{\btheta} \norm{\btheta}^2$, such that $y_i\btheta\cdot\bx_i \geq 1 (i = 1,\dots,n)$\\
\textbf{Soft margin SVM},\\
$\min_{\btheta} \norm{\btheta}^2 + C\sum_{i=1}^{n}(1-y_i\btheta\cdot\bx_i)_+$
\ruler
\textbf{Regularization and SVMs}:
Simulated data with many features $\phi(\bx)$;
C controls trade-off between margin $1/\norm{\btheta}$ and fit to data;
Large C: focus on fit to data (small margin is ok). More overfitting.
Small C: focus on large margin, less tendency to overfit.
Overfitting increases with: less data, more features.
\ruler
$\btheta = \sum_{j}\alpha_j y_j\bx_j$, $\alpha_j \neq 0$ only for support vectors.
\ruler
$K(\bx_i, \bx_j) = \phi(\bx_i)\cdot\phi(\bx_j)$, K is called a kernel.\\
Solve $\alpha_j$ to determine $\sum_j\alpha_j y_j\phi(\bx_j)$.\\
Compute the classifier for a test point $\bx$ via $\btheta\cdot\phi(\bx) = \sum_{j}\alpha_j y_j K(\bx_j, \bx)$
\ruler
degree-m polynomial kernel: $K_m(\bx, \tilde{\bx}) = (1+\bx\cdot\tilde{\bx})^m$\\
radial basis function kernel: $K_{rbf}(\bx, \tilde{\bx}) = \text{exp}(-\gamma\norm{\bx-\tilde{\bx}}^2)$
% --------------------------------

\section{Decision Theory}
\textbf{Loss function}: $l : \mathcal{Y} \times \mathcal{Y} \rightarrow \R$,
and $l(\hat{y}, y)$ is the cost of predicting $\hat{y}$ when the outcome is $y$.
\ruler
Assume $(\bX, \bY)$ are chosen i.i.d according to some probability distribution on $\mathcal{X}\times\mathcal{Y}$.
\textbf{Risk} is misclassification probability: $R(f) = \E l(f(\bX), \bY) = Pr(f(\bX) \neq \bY)$
\ruler
\textbf{Bayes Decision Rule} is
$
f^{*}(x) =
\begin{cases}
    1,  & \text{if } P(\bY=1|x) > P(\bY=-1|x) \\
    -1, & \text{otherwise. }
\end{cases}
$,\\
and the optimal risk (Bayes risk) $R^{*} = \inf_{f}R(f)=R(f^*)$
\ruler
\textbf{Excess risk} is for any $f: \mathcal{X} \rightarrow \{-1, +1\}$,\\
$R(f) - R^* = \E (1[f(x)\neq f^*(x)]|2P(\bY=+1|\bX) - 1|)$
\ruler
\textbf{Risk in Regression} is expected  squared error:
$R(f) = \E l(f(\bX), \bY) = \E \E [f(\bX) - \bY^2 | \bX]$
\ruler
\textbf{Bias-variance decomposition}:
$R(f) = \E[\underbrace{(f(\bX) - \E[\bY|\bX])^2}_{\text{bias}^2}] + \E[\underbrace{(\E[\bY|\bX]-\bY)^2}_{\text{variance}}]$
% --------------------------------

\section{Generative and Discriminative}
\textbf{Discriminative models}: $P(\bX, \bY) = P(\bX)P(\bY|\bX)$.\\
Estimate $P(\bY|\bX)$, then pretend out estimate $\hat{P}(\bY|\bX)$ is the actual $P(\bY|\bX)$ and
plug in bayes rule expression.
\ruler
\textbf{Generative model}: $P(\bX, \bY) = P(\bY)P(\bX|\bY)$.\\
Estimate $P(\bY)$ and $P(\bX|\bY)$, then use bayes theorem to calculate $P(\bY|\bX)$ and use discriminative model.
% --------------------------------

\section{Estimation}
\textbf{Method of moments}:
Match moments of the distribution to momemnts measured in the data.
\ruler
\textbf{Maximum likelihood}:
Choose parameter so that the distribution it defines gives the obverved data the highest probability (likelihood).
\ruler
\textbf{Maximum log likelihood}:
Log of maximum likelihood, equilvalent to maximum likelihood since log is monotonically increase; it is useful
since it can change $\prod$ to $\sum$.
\ruler
\textbf{Penalized maximum likelihood}:
Add a penalty term in the maximum (log) likelihood equation; treat the penalty term as some imaginary data points
crafted for desired probability.
\ruler
\textbf{Bayesian estimate}:
Treat parameter as a random variable, then update based on observed value (data).\\
Prior: $\pi(p) = 1$,\\
Posterior: $P(p|\bX_1 = 1) = P(\bX_1 = 1|p)\pi(p) / \int P(X_1 = 1|q)d\pi(q)$
\ruler
\textbf{Maximum a posterior probability}: the mode of the posterior. If uniform prior, MAP is MLE; if not
uniform prior, MAP is Penalized MLE.
\\%\ruler
\textbf{Gaussian maximum likelihood estimation}:\\
$\mu = \frac{1}{n}\sum_{i=1}^{n}x_i$, $\sigma^2 = \frac{1}{n}\sum_{i=1}^{n}(x_i - \mu)^2$
% --------------------------------

\section{Multivariate Normal Distribution}
$\bx \in \R^d: p(x) = \frac{1}{(2\pi)^{d/2}|\bSigma|^{1/2}}e^{(-\frac{1}{2}\tp{(\bx-\bmu)}\inv{\bSigma}(\bx-\bmu))}$
\ruler
\textbf{Covariance matrix}:
$\bSigma = \E(\bX - \bmu)\tp{(\bX - \bmu)}$\\
Symmetric: $\bSigma_{i,j} = \bSigma_{j_i}$\\
Non-negative diagonal entries: $\bSigma{i,i} \geq 0$\\
Positive semidefinite: $\forall \bv \in \R^d, \tp{\bv}\bSigma\bv \geq 0$
\ruler
\textbf{Spectral Theorem} for \textbf{non-diagonal covariance}:\\
$U = [\bv_1, \bv_2, \dots, \bv_n], \bLambda = \text{diag}(\tp{[\lambda_1, \lambda_2, \dots, \lambda_n]})$\\
We can eigen decompose $\inv{\bSigma} = U\inv{\bLambda}\tp{U}$, this is like to change to a different eigen spaces, where covariances ($\bLambda$) diagonal axis-alianed.
\ruler
Assume independent,
$ \gaussian{\bmu_x}{\bSigma} + \gaussian{\bmu_y}{\bSigma_y} = \gaussian{\bmu_x+\bmu_y}{\bSigma_x+\bSigma_y}$
\ruler
Given a $d$-dimensaional Gaussian $\bX \sim \gaussian{\bmu}{\bSigma}$,\\
write $\bX = \left[ \begin{smallmatrix} \bY \\ \bZ \end{smallmatrix} \right]$,
$\bmu = \left[ \begin{smallmatrix} \bmu_\bY \\ \bmu_\bZ \end{smallmatrix} \right]$,
$\bSigma = \left[ \begin{smallmatrix} \bSigma_{\bY\bY} & \bSigma_{\bY\bZ} \\ \bSigma_{\bZ\bY} & \bSigma_{\bZ\bZ} \end{smallmatrix} \right]$,\\
where $\bY \in \R^m$, and $\bZ \in \R^{d-m}$. Then $\bY \sim \gaussian{\mu_\bY}{\bSigma_{\bY\bY}}$
\ruler
Given a $d$-dimensaional Gaussian $\bX \sim \gaussian{\bmu}{\bSigma}$,\\
matrix $\bA \in \R^{m\times d}$ and vector $\bb \in \R^m$, define $\bY = \bA\bX + \bb$.\\
Then $\bY \sim \gaussian{\bA\bmu + \bb}{\bA\bSigma\tp{\bA}}$
\ruler
Given a $d$-dimensaional Gaussian $\bX \sim \gaussian{\bmu}{\bSigma}$,\\
with $\bSigma$ positive definite,\\
$\bY = \bSigma^{-\frac{1}{2}}(\bX-\bmu) \sim \gaussian{\mathbf{0}, \bI}$
% \section{Regression Regularization}
% \section{Logistic Regression}
% \section{Linear Discriminant Analysis}
% \section{Convex Optimization}
\end{multicols*}
\end{document}
